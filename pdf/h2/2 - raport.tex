\documentclass{article}

\usepackage[margin=1.75cm]{geometry}
\usepackage{graphicx}
\usepackage{pgfplots}
\usepackage{epstopdf}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{amsfonts}

\usepackage{subcaption}
\usepackage{hyperref}
\usepackage{multirow}
\usepackage{inputenc}
\usepackage{babel}
\usepackage{ragged2e}

\usetikzlibrary{arrows.meta, patterns}
\newcommand{\ceil}[1]{\lceil #1 \rceil}

\pgfplotsset{compat=1.18}

\title{H2 – Testing standard genetic algorithm on numerical benchmark functions}
\author{Dincă Georgian(2E2) - Brahă Petru(2E3)}
\date{November 26, 2024}

%%------------------------------------------------
%% actual document:

\begin{document}

\maketitle

\begin{abstract}

This work contains an in-depth analysis of the standard genetic algorithm applied to four numerical optimization functions: Sphere, Rastrigin, Schwefel and Michalewicz. Each of them were evaluated across the following dimension-cases: 10, 30, and 100. In addition to the solution quality, the execution times were recorded. This paper compares the current findings with the previous experiment where the Hill Climbing method was used (alongside four different improvement strategy types). !actual results!

\end{abstract}

\section{Introduction}

This paper starts by presenting the implementation details and the methods adopted. Every disclosure about the choices done in the code will be covered in the second chapter. In the third one, there is a short description of the experiment, which includes insights about the parameters used. Following the technical part, you can find an extensive review of the outputs produced by the constructed program together with their interpretation subsections. The studied values will be then compared with the Hill Climbing's results, previously discussed in H1. Last but not least, the formulated conclusion will end the current article.\\

In brief, the genetic algorithm probabilistically returns good enough answers comparable with the optimal point. Their mechanism imitates the natural model explained by Darwin's theory: each individual fights for survival whereas only the fittest succeed. The beginning of its procedure initialize at random n chromosomes, which then are tested and modified throughout multiple generations. In every such iteration the superior representations are selected to replace those with poor fitness values, shaping the population. Searching for new candidates is accomplished by the cross-over and mutation, traditional operators. 

\paragraph{}
some observations and some results.

\subsection{Motivation}

Our investigation proposes a characterization of the genetic algorithm's behavior. This topic is a matter of our laboratory work during the university studies. De Jong advised to not "perceive GAs themselves as optiomization algorithms" \cite{dejong}; our work explores the evolutionary tool regarding his statement.

%%------------------------------------------------
%% explanations:

\section{Method}

\subsection{Hardware}

\paragraph{}
The algorithms have been implemented to leverage GPU acceleration using NVIDIA's CUDA framework. The system used to run the tests was equipped with an RTX 4090 graphics card, mobile version. The parallel nature of local search algorithms, where multiple independent search iterations can be executed simultaneously, makes them particularly well-suited for GPU implementation due to their architecture having a huge number of cores and threads that can run in parallel. Each search iteration operates independently on its own thread, which parallelizes the workload efficiently.

\paragraph{}
Our GPU implementation utilizes a thread organization scheme of 32 threads per block, aligning with the NVIDIA warp size for optimal execution efficiency. This configuration minimizes thread divergence and maximizes memory coalescing, concluding in improved computational performance.

\paragraph{}
To attempt function evaluation, we employed two popular optimization algorithms: Hill Climbing and Simulated annealing (hybridization of HC best improvement); both are covered in the next subsections.

\subsection{Gentic algorithm implementation}

\paragraph{}
The primary goal is to identify a peak solution by continually moving towards neighboring solutions with higher fitness outcomes. A neighbor of a bit string is a copy of the current representation but with only one flipped bit. In other words, for every bit there is associated with a neighbor; this can be understood as an iteration of the string. For only one run of the algorithm, improvements are performed until there is no better value in the neighborhood - the end condition. The real values are then computed and the result of the benchmark functions concerning these computations is returned.

\subsection{Simulated annealing implementation}

\paragraph{}
Our implementation features a hybrid approach that combines traditional SA with a Best Improvement local search phase after the temperature drops and it can no longer escape the local optimum. The initial temperature $T_0$ is dynamically calculated based on the dimension number using the formula $T_0 = \left|\frac{fmax \cdot n}{\ln(0.8)}\right|$, where $n$ is the number of dimensions. $fmax$ is the aproximative maximum value of the function for one dimension and assures that the temperature can be high enough for exploration:
\begin{itemize} 
    \item Sphere's is very simple and doesn't need optimized temperature initialization
    \item Rastrigin's - 40 
    \item Schwefel's - 840 
    \item Michalewicz's - 2 
\end{itemize} 
This adaptive initial temperature ensures appropriate scaling of the acceptance probability across different problem dimensions. 

\paragraph{}
The cooling schedule is implemented with a geometric decay and a cooling rate of 0.985, maintaining a balance between exploration and exploitation. The algorithm progresses until either the temperature falls below a threshold of $T_0\times10^{-8}$, or when the search stagnates for four consecutive temperature changes. For each temperature, we attempt to achieve $20\cdot n$ successful moves, where n is the number of dimensions, with a maximum of $200\cdot n$ total attempts per temperature to prevent excessive computation in flat regions of the search space. 

\paragraph{}
The solution representation uses a binary encoding, with neighborhood moves implemented as single bit-flips. Move acceptance follows the standard Metropolis \cite{m} criterion:
\begin{itemize}
    \item Improvements are always accepted
    \item Deteriorating moves are accepted with probability $\exp\left(-\frac{| \Delta f |}{T}\right)$, where $\Delta f$ is the absolute fitness difference and T is the current temperature
\end{itemize}

\section{Experiment's description}

To ensure precise outcomes, we increased the number of decimal places after 0, from five to seven. This approach provided even stronger numbers with meaningless computation cost. Similarly with the previous analysis - H1 - the 10, 30, and 100 dimensions of the numerical functions were examined; this allows us to firmly state the differences obtained from both statistics.

The population is formed out of 100 chromosomes which 
pass through 20.000 generations. The probability of reproduction is 0.25 with five cutting points and the mutation rate is 0.01 per chromosome.

%%------------------------------------------------
%% end of presentation:

\newpage
\section{Experiment's results}

\subsection{Sphere}
 
\begin{table}[htbp]
\begin{minipage}{.4\linewidth}
    \centering

    \begin{tabular}{|c|c|c|c|c|}
    \hline
    D   & $\sigma$  & avg. time     & avg. sol.     & best sol. \\
    \hline
    10  & 0         & 89ms          & 0       & 0 \\
    \hline
    30  & 0         & 2.36s         & 0       & 0 \\
    \hline
    100 & 0         & 61.539s       & 0       & 0 \\
    \hline
    \end{tabular}
    \caption{Best improvement}
  \end{minipage}%
  \quad % ----------------------------------
  \begin{minipage}{.75\linewidth}
    \centering

    \begin{tabular}{|c|c|c|c|c|}
    \hline
    D   & $\sigma$  & avg. time     & avg. sol.     & best sol. \\
    \hline
    10  & 0         & 50ms          & 0             & 0 \\
    \hline
    30  & 0         & 1.21s         & 0             & 0 \\
    \hline
    100 & 0         & 32.58s        & 0             & 0 \\
    \hline
    \end{tabular}
    \caption{First improvement}
  \end{minipage}
\end{table}
\begin{table}[!htbp]
\begin{minipage}{.4\linewidth}
    \centering

    \begin{tabular}{|c|c|c|c|c|}
    \hline
    D   & $\sigma$  & avg. time     & avg. sol.     & best sol. \\
    \hline
    10  & 0         & 200ms         & 0             & 0 \\
    \hline
    30  & 0         & 4.98s         & 0             & 0 \\
    \hline
    \end{tabular}
    \caption{Worst improvement}
  \end{minipage}%
  \quad % ----------------------------------
  \begin{minipage}{.75\linewidth}
    \centering

    \begin{tabular}{|c|c|c|c|c|}
    \hline
    D   & $\sigma$  & avg. time     & avg. sol.     & best sol. \\
    \hline
    10  & 0         & 310ms         & 0             & 0 \\
    \hline
    30  & 0         & 4.97s         & 0             & 0 \\
    \hline
    100 & 0         & 152.95s       & 0             & 0 \\
    \hline
    \end{tabular}
    \caption{Simulated annealing}
  \end{minipage}
\end{table}

\newpage
\setcounter{table}{0}

%%------------------------------------------------
\subsection{Rastrigin}
    
\begin{table}[htbp]
\begin{minipage}{.4\linewidth}
    \centering
    
    \begin{tabular}{|c|c|c|c|c|}
    \hline
    D   & $\sigma$  & avg. time     & avg. sol.     & best sol.\\
    \hline
    10  & 0.53      & 307ms         & 1.96332       & 0.99496 \\
    \hline
    30  & 2.30      & 7.74s         & 22.49692      & 16.15900 \\
    \hline
    100 & 5.25      & 271.91s       & 124.04872     & 103.27991 \\
    \hline
    \end{tabular}
    \caption{Best improvement}
  \end{minipage}%
  \quad % ----------------------------------
  \begin{minipage}{.75\linewidth}
    \centering
    
    \begin{tabular}{|c|c|c|c|c|}
    \hline
    D   & $\sigma$  & avg. time     & avg. sol.     & best sol. \\
    \hline
    10  & 0.79      & 182ms         & 3.42327       & 1.23582 \\
    \hline
    30  & 2.63      & 4.37s         & 29.97694      & 22.84631 \\
    \hline
    100 & 4.99      & 154.12s       & 155.08213     & 141.60620 \\
    \hline
    \end{tabular}
    \caption{First improvement}
  \end{minipage}
\end{table}
\begin{table}[!htbp]
\begin{minipage}{.4\linewidth}
    \centering

    \begin{tabular}{|c|c|c|c|c|}
    \hline
    D   & $\sigma$  & avg. time     & avg. sol.     & best sol. \\
    \hline
    10  & 0.91      & 2.51s         & 5.12165       & 3.00004 \\
    \hline
    30  & 3.23      & 57.13s        & 38.18328      & 29.23457 \\
    \hline
    \end{tabular}
    \caption{Worst improvement}
  \end{minipage}%
  \quad % ----------------------------------
  \begin{minipage}{.75\linewidth}
    \centering

    \begin{tabular}{|c|c|c|c|c|}
    \hline
    D   & $\sigma$  & avg. time     & avg. sol.     & best sol. \\
    \hline
    10  & 0         & 19.81s        & 0       & 0 \\
    \hline
    30  & 0.86      & 171.31s       & 6.20453       & 3.99499 \\
    \hline
    100 & 1.94      & 19.16min      & 47.64466      & 40.26256 \\
    \hline
    \end{tabular}
    \caption{Simulated annealing}
  \end{minipage}
\end{table}

\newpage
\setcounter{table}{0}

%%------------------------------------------------

\subsection{Schwefel}

\begin{table}[htbp]
\begin{minipage}{.4\linewidth}
    \centering

    \begin{tabular}{|c|c|c|c|c|}
    \hline
    D   & $\sigma$  & avg. time     & avg. sol.     & best sol. \\
    \hline
    10  & 18.4      & 755ms         & 17.70524      & 0.20936 \\
    \hline
    30  & 98.35     & 19.813s       & 894.62572     & 696.92139 \\
    \hline
    100 & 230.35    & 7,62min       & 5747.60089    & 5326.42004 \\
    \hline
    \end{tabular}
    \caption{Best improvement}
  \end{minipage}%
  \quad % ----------------------------------
  \begin{minipage}{.75\linewidth}
    \centering

    \begin{tabular}{|c|c|c|c|c|}
    \hline
    D   & $\sigma$  & avg. time     & avg. sol.     & best sol. \\
    \hline
    10  & 45.79     & 434ms         & 120.65099     & 34.54979 \\
    \hline
    30  & 111.23    & 11.18s        & 1464.46143    & 1186.97187 \\
    \hline
    100 & 217.08    & 261.82s       & 7615.11165    & 6976.11061 \\
    \hline
    \end{tabular}
    \caption{First improvement}
  \end{minipage}
\end{table}
\begin{table}[!htbp]
\begin{minipage}{.4\linewidth}
    \centering

    \begin{tabular}{|c|c|c|c|c|}
    \hline
    D   & $\sigma$  & avg. time     & avg. sol.     & best sol. \\
    \hline
    10  & 17.35     & 7.59s         & 212.34472     & 161.40279 \\
    \hline
    30  & 108.91    & 118.25s       & 1550.94308    & 1262.86526 \\
    \hline
    \end{tabular}
    \caption{Worst improvement}
  \end{minipage}%
  \quad % ----------------------------------
  \begin{minipage}{.75\linewidth}
    \centering

    \begin{tabular}{|c|c|c|c|c|}
    \hline
    D   & $\sigma$  & avg. time     & avg. sol.     & best sol. \\
    \hline
    10  & 0         & 22.98s        & 0.00037       & 0.00014 \\
    \hline
    30  & 0.07      & 173.01s       & 0.38921       & 0.21216 \\
    \hline
    100 & 41.02     & 25.93min      & 126.46816     & 71.91137 \\
    \hline
    \end{tabular}
    \caption{Simulated annealing}
  \end{minipage}
\end{table}

\newpage
\setcounter{table}{0}

%%------------------------------------------------

\subsection{Michalewicz}

\begin{table}[htbp]
\begin{minipage}{.4\linewidth}
    \centering

    \begin{tabular}{|c|c|c|c|c|}
    \hline
    D   & $\sigma$  & avg. time     & avg. sol.     & best sol. \\
    \hline
    10  & 0.04      & 1.50s         & -9.56506      & -9.65221 \\
    \hline
    30  & 0.28      & 33.42s        & -27.61249     & -28.56555 \\
    \hline
    100 & 0.51      & 9.73min       & -88.04727     & -89.11685 \\
    \hline
    \end{tabular}
    \caption{Best improvement}
  \end{minipage}%
  \quad % ----------------------------------
  \begin{minipage}{.75\linewidth}
    \centering

    \begin{tabular}{|c|c|c|c|c|}
    \hline
    D   & $\sigma$  & avg. time     & avg. sol.     & best sol. \\
    \hline
    10  & 0.05      & 865ms         & -9.50893      & -9.61160 \\
    \hline
    30  & 0.18      & 19.48s        & -27.02483     & -27.45056 \\
    \hline
    100 & 0.37      & 6.41min       & -73.07365     & -77.4654  \\
    \hline
    \end{tabular}
    \caption{First improvement}
  \end{minipage}
\end{table}
\begin{table}[!htbp]
\begin{minipage}{.4\linewidth}
    \centering

    \begin{tabular}{|c|c|c|c|c|}
    \hline
    D   & $\sigma$  & avg. time     & avg. sol.     & best sol. \\
    \hline
    10  & 0.12      & 7.39s         & -9.22340      & -9.42528 \\
    \hline
    30  & 0.29      & 104.44s       & -24.57051     & -25.19599 \\
    \hline
    \end{tabular}
    \caption{Worst improvement}
  \end{minipage}%
  \quad % ----------------------------------
  \begin{minipage}{.75\linewidth}
    \centering

    \begin{tabular}{|c|c|c|c|c|}
    \hline
    D   & $\sigma$  & avg. time     & avg. sol.     & best sol. \\
    \hline
    10  & 0         & 24.69s        & -9.65930      & -9.66015 \\
    \hline
    30  & 0.07      & 210.07s       & -29.11762     & -29.27980 \\
    \hline
    100 & 0.17      & 37.3min       & -95.99791     & -96.32860 \\
    \hline
    \end{tabular}
    \caption{Simulated annealing}
  \end{minipage}
\end{table}

%%------------------------------------------------

\subsection{Interpretation}

\paragraph{}
The experimental results demonstrate that \textit{\textbf{Best Improvement}} consistently outperforms \textit{\textbf{First Improvement}} in solution quality across all test functions, particularly in higher dimensions, however at a higher computational cost. \textit{\textbf{Simulated annealing}} exhibits superior performance by escaping local optima, especially notable for \textbf{Rastrigin} and \textbf{Schwefel} functions, yielding better outputs when compared against \textit{\textbf{Best Improvement}}. Finally, \textit{\textbf{Worst Improvement}} showed inferior results at a higher computational cost than all other strategies.

\section{Conclusions}

\paragraph{}
Looking into our results, it can safely be stated that the Hill Climbing algorithm is efficient and succeeds in delivering significant values, in the search for the global minimum. Comparing it with the deterministic approach, the time is much less than the brute-force traversal of the graph function. The different improvement settings showcase diverse trade-offs between these designs which can be further exploited. \\

An exemplary observation would be the Simulated annealing contributions; they usually bring better solutions that are scalable for real competitive environments. It's useful in concrete cases, even though considerable time intervals were detected. \\

Overall, this study provides valuable insights into the application of optimization algorithms in solving complex optimization problems and contributes to the understanding of their performance across different dimensions. 

%%------------------------------------------------
%% bibliography

\begin{thebibliography}{9}

\bibitem{michalewicz} Michalewicz Zbigniew. "Genetic algorithms + data structures = evolution programs". Springer Science \& Business Media, 2013.

\bibitem{dejong} De Jong, Kenneth. "Genetic algorithms: A 10 year perspective." Proceedings of the first International Conference on Genetic Algorithms and their Applications. Psychology Press, 2014.

\bibitem{course}
  Croitoru Eugen. Genetic algorithms course. Alexandru Ioan Cuza University, 2024.\\
  \url{https://profs.info.uaic.ro/eugen.croitoru/teaching/ga/}

\bibitem{uni-sp} Surjanovic Sonja and Bingham Derek. Sphere's function. Simon Fraser University, 2013.\\
  \url{https://www.sfu.ca/~ssurjano/spheref.html}

\bibitem{uni-ra} Surjanovic Sonja and Bingham Derek. Rastrigin's function. Simon Fraser University, 2013.\\
  \url{https://www.sfu.ca/~ssurjano/rastr.html}

\bibitem{uni-sw} Surjanovic Sonja and Bingham Derek. Schwefel's function. Simon Fraser University, 2013.\\
  \url{https://www.sfu.ca/~ssurjano/schwef.html}

\bibitem{uni-mc} Surjanovic Sonja and Bingham Derek. Michalewicz's function. Simon Fraser University, 2013.\\
  \url{https://www.sfu.ca/~ssurjano/michal.html}

\bibitem{m}
    Metropolis \url{https://www.sciencedirect.com/topics/computer-science/metropolis-algorithm} \\  

\bibitem{l}
  Overleaf \\ LaTeX training.
  \url{https://tex.stackexchange.com/questions/39017/how-to-influence-the-position-of-float-environments-like-figure-and-table-in-lat/39020#39020} \\
  \url{https://latex-cookbook.net/function-plot/%7D} \\  
  \url{https://www.overleaf.com/learn/latex/Learn_LaTeX_in_30_minutes%7D}

\end{thebibliography}  
\end{document}
